{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules first\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 4096\n",
    "load_in_4bit = False\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"baidu/ERNIE-4.5-0.3B-Base-PT\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    dtype=torch.bfloat16,\n",
    "    full_finetuning=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5707dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=64,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753f1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = load_dataset(\"TeichAI/gemini-3-flash-preview-1000x\", split=\"train\")\n",
    "dataset_2 = load_dataset(\"TeichAI/claude-haiku-4.5-1700x\", split=\"train\")\n",
    "dataset_3 = load_dataset(\"TeichAI/gemini-2.5-flash-lite-2509-preview-1000x\", split=\"train\")\n",
    "\n",
    "dataset = concatenate_datasets([dataset_1, dataset_2, dataset_3])\n",
    "\n",
    "\n",
    "chat_template = \"\"\"<|im_start|>system\n",
    "{}<|im_end|>\n",
    "<|im_start|>user\n",
    "{}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{}<|im_end|>\"\"\"\n",
    "\n",
    "def apply_chat_template(row):\n",
    "    text = row[\"messages\"]\n",
    "    input_text = text[0][\"content\"]\n",
    "    target_text = text[1][\"content\"]\n",
    "\n",
    "    return {\"text\": chat_template.format(\n",
    "        \"You are a helpful assistant that provides detailed and accurate responses based on the user's input.\",\n",
    "        input_text,\n",
    "        target_text\n",
    "    )}\n",
    "\n",
    "\n",
    "dataset = dataset.map(apply_chat_template, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=True,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=5,\n",
    "        learning_rate=5e-5,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_steps=50,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "    ),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides detailed and accurate responses based on the user's input.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Make a python script for fizz buzz problem.\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(input_ids, streamer=text_streamer, max_new_tokens=512, pad_token_id=tokenizer.eos_token_id, temperature=0.7, top_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee0799",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"finetuned_model\")\n",
    "tokenizer.save_pretrained(\"finetuned_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
