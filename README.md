# ERNIE-4.5 Fine-tuning Project

This project contains a fine-tuned version of the Baidu ERNIE-4.5 model (0.3B-Base-PT) trained on high-quality conversational datasets.

## üìã Project Overview

This is a supervised fine-tuning (SFT) project that adapts the ERNIE-4.5 language model for improved conversational abilities using instruction-following data from multiple sources.

---

## üìä Datasets

The model was trained on a combination of three high-quality instruction datasets:

| Dataset | Source | Size | Description |
|---------|--------|------|-------------|
| Gemini 3 Flash Preview | TeichAI/gemini-3-flash-preview-1000x | ~1,000 samples | Responses generated by Google's Gemini 3 Flash model |
| Claude Haiku 4.5 | TeichAI/claude-haiku-4.5-1700x | ~1,700 samples | Responses generated by Anthropic's Claude Haiku 4.5 |
| Gemini 2.5 Flash Lite | TeichAI/gemini-2.5-flash-lite-2509-preview-1000x | ~1,000 samples | Responses generated by Google's Gemini 2.5 Flash Lite model |

**Total**: ~3,700 examples (training set: 90%, evaluation set: 10%)

### Data Format

Each example consists of:
- **System prompt**: "You are a helpful assistant that provides detailed and accurate responses based on the user's input."
- **User message**: The input instruction or question
- **Assistant response**: The target completion

Messages are formatted using a standard chat template:
```
<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{user_message}<|im_end|>
<|im_start|>assistant
{assistant_response}<|im_end|>
```

---

## üèóÔ∏è Model Architecture

**Base Model**: Baidu ERNIE-4.5-0.3B-Base-PT

### Model Configuration

| Parameter | Value |
|-----------|-------|
| Architecture | Ernie4_5ForCausalLM |
| Model Type | Causal Language Model |
| Hidden Size | 1,024 |
| Number of Layers | 18 |
| Number of Attention Heads | 16 |
| Key-Value Heads | 2 |
| Head Dimension | 128 |
| Intermediate Size | 3,072 |
| Hidden Activation | SiLU |
| Vocabulary Size | 103,424 |
| Max Sequence Length | 131,072 |
| Data Type | bfloat16 |

### Fine-tuning Method: LoRA

**Low-Rank Adaptation (LoRA)** was used to efficiently fine-tune the model with minimal parameter overhead.

| Parameter | Value |
|-----------|-------|
| LoRA Rank (r) | 16 |
| LoRA Alpha | 64 |
| Scaling Factor | 4.0 (alpha / rank) |
| Target Modules | q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj |
| Bias Configuration | none |
| RsLoRA | False |
| Gradient Checkpointing | Enabled (unsloth variant) |

---

## ‚öôÔ∏è Training Hyperparameters

### Optimizer & Learning Rate Schedule

| Parameter | Value |
|-----------|-------|
| Optimizer | AdamW 8-bit |
| Base Learning Rate | 5.0 √ó 10‚Åª‚Åµ |
| Learning Rate Scheduler | Cosine Annealing |
| Weight Decay | 0.01 |
| Gradient Clipping | Enabled |

### Batch & Gradient Configuration

| Parameter | Value |
|-----------|-------|
| Per-device Train Batch Size | 32 |
| Gradient Accumulation Steps | 2 |
| Effective Batch Size | 64 |
| Max Sequence Length | 4,096 |
| Packing | Enabled |

### Training Schedule

| Parameter | Value |
|-----------|-------|
| Number of Epochs | 5 |
| Total Training Steps | 260 |
| Warmup Steps | 100 |
| Evaluation Interval | Every 50 steps |
| Checkpoint Interval | Every 50 steps |
| Max Checkpoints Kept | 3 |
| Early Stopping Patience | 5 epochs without improvement |

### Evaluation Metrics

| Metric | Best Value | Best Step |
|--------|-----------|----------|
| Evaluation Loss | 1.289 | Step 100 |

---


## üìà Training Results

### Loss Progression

- **Initial Loss (Step 10)**: 1.434
- **Best Evaluation Loss (Step 100)**: 1.289
- **Final Loss (Step 260)**: ~1.26 (approximately, from log history trends)

The model showed consistent improvement across 5 epochs with early stopping triggered after step 100 (best performing checkpoint).

### Key Observations

- Rapid loss convergence in the first epoch
- Best performance achieved at step 100 (epoch ~1.9)
- Cosine annealing learning rate helped stabilize training
- Gradient norms decreased as training progressed, indicating improved model stability

---

## üõ†Ô∏è Dependencies

Key libraries used in this project:

- **unsloth**: Fast fine-tuning framework with Flash Attention
- **transformers**: HuggingFace transformers library
- **trl**: Training libraries for language models (SFTTrainer)
- **torch**: PyTorch framework
- **datasets**: Dataset loading and processing

See the training notebook for the complete dependency list.

---


## üîó References

- [Baidu ERNIE-4.5](https://huggingface.co/baidu/ERNIE-4.5-0.3B-Base-PT)
- [Unsloth GitHub](https://github.com/unslothai/unsloth)
- [TRL Documentation](https://huggingface.co/docs/trl/)
- [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)

---

**Created**: December 2025  
**Training Framework**: Unsloth + TRL  